{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68bb7ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as py\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03a53111",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/adityagoyal/Desktop/ggml/diabetes_prediction_dataset.csv') as f:\n",
    "    df = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bf67fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {'No Info': np.int64(0), 'current': np.int64(1), 'ever': np.int64(2), 'former': np.int64(3), 'never': np.int64(4), 'not current': np.int64(5)}\n"
     ]
    }
   ],
   "source": [
    "le_smoker = LabelEncoder()\n",
    "df['smoking_history'] = le_smoker.fit_transform(df['smoking_history'])\n",
    "print(\"Label mapping:\", dict(zip(le_smoker.classes_, le_smoker.transform(le_smoker.classes_))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "575eaf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {np.int64(0): np.int64(0), np.int64(1): np.int64(1), np.int64(2): np.int64(2)}\n"
     ]
    }
   ],
   "source": [
    "le_gender = LabelEncoder()\n",
    "df['gender'] = le_gender.fit_transform(df['gender'])\n",
    "print(\"Label mapping:\", dict(zip(le_gender.classes_, le_gender.transform(le_gender.classes_))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47e92ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:len(df.columns)-1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e60b36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.iloc[:,-1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7c1caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(x,y):\n",
    "  x_train,x_,y_train,y_ = train_test_split(x,y,train_size=0.7,random_state=1)\n",
    "  x_cv , x_test , y_cv, y_test = train_test_split(x_,y_,train_size=0.5,random_state=1)\n",
    "  return x_train,y_train,x_cv,y_cv,x_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4102d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def modelF(x_train,y_train,learningRate, regRate):\n",
    "\n",
    "  model = Sequential([\n",
    "    Dense(units = 4, activation = 'relu',kernel_regularizer=regularizers.l2(regRate)),\n",
    "    Dense(units = 4, activation = 'relu',kernel_regularizer=regularizers.l2(regRate)),\n",
    "    Dense(units = 2, activation = 'softmax',kernel_regularizer=regularizers.l2(regRate))\n",
    "    ]\n",
    "   )  \n",
    "\n",
    "  model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=learningRate),loss = SparseCategoricalCrossentropy())\n",
    "  model.fit(x_train,y_train,epochs = 50)\n",
    "  \n",
    "  return learningRate, regRate, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee75b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndCv(model,x_cv,y_cv,x_train,y_train):\n",
    "\n",
    "  y_train_prob = model.predict(x_train)\n",
    "  y_train_pred = np.zeros(len(y_train_prob))\n",
    "\n",
    "  y_cv_prob = model.predict(x_cv)\n",
    "  y_cv_pred = np.zeros(len(y_cv_prob))\n",
    "\n",
    "  j=0\n",
    "  for i in y_train_prob:\n",
    "    y_train_pred[j] = np.argmax(i)\n",
    "    j+=1\n",
    "    \n",
    "  j=0\n",
    "  for i in y_cv_prob:\n",
    "    y_cv_pred[j] = np.argmax(i)\n",
    "    j+=1\n",
    "    \n",
    "  y_train_pred = y_train_pred.reshape(len(y_train_prob),1)\n",
    "  y_cv_pred = y_cv_pred.reshape(len(y_cv_prob),1)\n",
    "\n",
    "  train_err = np.mean(y_train_pred!=y_train)\n",
    "  cv_err = np.mean(y_cv_pred!=y_cv)\n",
    "\n",
    "  return train_err,cv_err, y_train_pred,y_cv_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f5f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testError(model,x_test,y_test):\n",
    "    y_test_prob = model.predict(x_test)\n",
    "    y_test_pred = np.zeros(len(y_test_prob))\n",
    "    \n",
    "    i=0\n",
    "    for j in y_test_prob:\n",
    "        y_test_pred[i] = np.argmax(j)\n",
    "        i+=1\n",
    "    y_test_pred = y_test_pred.reshape(len(y_test_prob),1)\n",
    "    test_err = np.mean(y_test_pred!=y_test)\n",
    "    return test_err,y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b975c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestModel(modelErrors,overfitCriteria):\n",
    "    \n",
    "    errors = []\n",
    "    dic = defaultdict(list)\n",
    "    \n",
    "    # computing the mse errors\n",
    "    for i in modelErrors:\n",
    "        \n",
    "        train_err = i[0]\n",
    "        cv_err = i[1]\n",
    "        test_err = i[2]\n",
    "        \n",
    "        error = (train_err-cv_err )**2 + (train_err-test_err )**2   # test and cv error is always almost same , so no point taking diff of that\n",
    "        errors.append(error)\n",
    "    \n",
    "    \n",
    "    # integrating mses to their corresponding indexes\n",
    "    j=0\n",
    "    for i in errors:\n",
    "        dic[i].append(j)\n",
    "        j+=1\n",
    "    \n",
    "    \n",
    "    # sorting the mses from least to largest\n",
    "    sortedLise = sorted(dic.items())\n",
    "        \n",
    "    # seprate all indexes of models that have low variance\n",
    "    fitLise = []\n",
    "    for i in sortedLise:\n",
    "        if(i[0]<overfitCriteria):\n",
    "            for j in i[1]:\n",
    "              fitLise.append(j)\n",
    " \n",
    " \n",
    "     # iterate through all the indexes and check if has least cv error\n",
    "    cv_error = 100\n",
    "    index = -1\n",
    "    \n",
    "    for i in fitLise:\n",
    "        if modelErrors[i][1] <cv_error:\n",
    "            cv_error = modelErrors[i][1] \n",
    "            index = i   \n",
    "\n",
    "    # return index of the best model\n",
    "    return index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0167b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_LEARNING_RATES = [0.01,0.02]\n",
    "SAMPLE_REG_RATES = [0.01,0.03]\n",
    "models = []\n",
    "OVERFIT_CRITERIA = 28\n",
    "\n",
    "ARRAY_SIZE = len(SAMPLE_LEARNING_RATES)*len(SAMPLE_REG_RATES)\n",
    "\n",
    "train_errors = np.zeros(ARRAY_SIZE)\n",
    "cv_errors = np.zeros(ARRAY_SIZE)\n",
    "test_errors =  np.zeros(ARRAY_SIZE)\n",
    "model_errors = np.zeros((ARRAY_SIZE,3))\n",
    "learningRates = np.zeros(ARRAY_SIZE)\n",
    "regRates = np.zeros(ARRAY_SIZE)\n",
    "\n",
    "i=0\n",
    "for j in SAMPLE_LEARNING_RATES:\n",
    "    for k in SAMPLE_REG_RATES:\n",
    "        x_train,y_train,x_cv,y_cv,x_test,y_test = split(X,y)\n",
    "        lRate,regRate,model = modelF(x_train,y_train,j,k)\n",
    "        train_err,cv_err,train_pred,cv_pred =  trainAndCv(model,x_cv,y_cv,x_train,y_train)\n",
    "        test_err,test_pred =                   testError(model,x_test,y_test)\n",
    "        models.append(model)\n",
    "        \n",
    "        test_errors[i] = test_err\n",
    "        train_errors[i] = train_err\n",
    "        cv_errors[i] = cv_err\n",
    "        model_error = [train_err,cv_err,test_err]\n",
    "        model_errors[i] = model_error\n",
    "        \n",
    "        learningRates[i] = lRate\n",
    "        regRates[i] = regRate   \n",
    "        \n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86ead16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Errors of all the models: \\n{model_errors}\\n\")\n",
    "\n",
    "indexOfBest = bestModel(model_errors,OVERFIT_CRITERIA)\n",
    "\n",
    "print(f\"Summary of the best model: \")\n",
    "models[indexOfBest].summary()\n",
    "\n",
    "print(\"Additional info about best model-: \\n\")\n",
    "print(f\"Error: {model_errors[indexOfBest]}\")\n",
    "print(f\"Index: {indexOfBest}\")\n",
    "print(f\"Learning Rate: {learningRates[indexOfBest]}\")\n",
    "print(f\"regularization rate: {regRates[indexOfBest]}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3632d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3004cad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl','wb') as f:\n",
    "    pl.dump(models[indexOfBest],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "15fbd4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl','rb') as f:\n",
    "    model = pl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3f33a239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.99656624, 0.00343379]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array([[0,25,1,1,4,28.9,6.5,120]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b37e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48a40957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "blood_glucose_level\n",
       "280    729\n",
       "160    696\n",
       "130    692\n",
       "300    674\n",
       "159    666\n",
       "145    662\n",
       "200    647\n",
       "126    636\n",
       "240    636\n",
       "260    635\n",
       "140    625\n",
       "220    603\n",
       "155    599\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['diabetes']==1]['blood_glucose_level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "94f7ab5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe14ce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# each of elements int he array rerpesents the wiehgt and bias\n",
    "# each row repreosents the weights for that prev unit.\n",
    "# the column rperessents a current units weights\n",
    "# relu(matrix.T @ input + col(b)) # 2 times  (2 hidden layers)\n",
    "# softmax(matrix.T @ input + col(b)) # last time (1 output layer)\n",
    "\n",
    "# softmax(arr[])\n",
    "# exp power of the logits \n",
    "# ei/sum of e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ec5697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (softmax)(2 into 4)(relu)(4 into 4)(relu)(4 into 8) (8 into 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6db221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ggml_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb265f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# firs tof all comptuation has to be done layer by layer in ggml fo the nueral network i a trying to implement...for a aproucalr layer, what would be the different if i intialise different result tensor \n",
    "# emtdaat for each unit instrad of together...\n",
    "\n",
    "# good question to think about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e82103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# does 2d work with 1d\n",
    "# is 1 d row or column orientated by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ca24fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.transpose(model.get_weights()[5]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b577de",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.transpose(model.get_weights()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb86b79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
